
---
title: "STAT340 Lecture 4 : The Logic of MC Testing"
output: html_document
---

```{r,echo=F}
knitr::opts_chunk$set(cache=T,message=F,warning=F)
```

<!-- Recall the "hot hands" example from the Monte Carlo section. How did we meaningfully determine if the player did or did not have hot hands in that example? -->

<!-- First, we **assumed throws were identical and independent**. Then we noted that under this assumption, we can make certain predictions about the data. In particular, we noted that this assumption implied that the sequence of throws was unimportant. Thus, the **throws can be arbitrarily shuffled without changing any statistical properties** of the data. Then, it was fairly easy to generate new, randomly shuffled versions (i.e. ***permutations***) of the data and compare a particular property of the original data v. the permuted data. -->

<!-- What we did may seem fairly ad-hoc, however this actually closely follows the standard procedure used in testing called the **Neyman-Pearson framework** -->

(these notes contain significant contributions from Karl Rohe)

```{r}
library(dplyr)
library(ggplot2)

longestRun = function(flips){
  # the first flip is always a run of length 1.
  MaxRunSoFar = 1
  currentRun = 1
  
  for(i in 2:length(flips)){ # for every flip
    # if it is equal to the last flip
    if(flips[i]==flips[i-1]){
      # then increase the length of the currentRun
      currentRun = currentRun + 1
      # and if the run is larger than the maxRunSoFar, redefine that.
      if(currentRun>MaxRunSoFar) MaxRunSoFar = currentRun
    }
    # otherwise, 
    if(flips[i]!=flips[i-1]){
      # set the current run back to 1
      currentRun=1
  }
  }
  return(MaxRunSoFar)
}

```

## Example: Fraud detection in coin flips

Suppose one of your classmates gave you a sequence of 200 H's and T's and you wanted to do "fraud detection" to see if it was generated "by hand" or with a random number generator.  


```{r}
sequence_of_flips = "THTHHHTTTTHTTTHTTHTHTTTTHHHTHHHTHTHHTHHHTHTTHHHTTTHTTTHHHTHTTTHTHHHTHTHTHHTTTHTHTHTTTHTHHHTTHTHHHHTHTTHTHTTHTHHHTHTHHTHTHTHHTHHHHTTHHTTHTTTHHHTHHHTHHTHHTHTTHTHHTHHHTHTHHHHTTHHHHTTTHTTHHHHTTHHTTTTHHHHH"
```


We could do this by testing the hypothesis

\[H_0: \mbox{Each coin flip is independent Bernoulli($p=1/2$)}\]

The null hypothesis is **a possible** statistical model for the data we observed; often called the "null model".  Now, we want to examine whether there is a "pattern in our data" that would be unlikely "under the null".  Why? Be sure to read the excerpt from Jordan Ellenberg's book!

There are lots of "patterns" or test statistics we could examine.  For example, "the proportion of H's" is the one you maybe learned about in a previous statistics course (this statistic has the advantage of converging to the normal distribution, i.e. Z-table and 1.96, etc).  

Because we suspect that the sequence was generated by hand, the proportion of H's might not be a very good test statistic; it is easily "gamed" by the person writing it.  It is much harder to "game" the length of the longest sequence.  This is because humans have a hard time imagining what that length should be. As a result, fraudulent sequences have runs which tend to be too short.  

So, the pattern in our data that we are going to examine is the length of the longest run.  This will be our *test statistic*.  We will call the test statistic $S$ when it is a random variable and we will call it $s$ when it is the observed value from our data.  Capital letters for things we simulate, lower case letters for things from our data. So, $S$ is a random variable that we can simulate and $s$ is our data.

In the .Rmd file for this page, there is a function `longestRun()`.  It counts the length of the longest run.  For the sequence above...

```{r}
s= sequence_of_flips %>% strsplit( split = "") %>% unlist() %>% longestRun()
s
```

How surprising would this value be if our data was generated from the null model? 



In homework 2, we defined a random variable $X$ that is the length of the longest run in a sequence of 200 independent, fair coin flips (i.e. exactly the null above).  Now, we are going to call this random variable $S$ because we imagine it as a test statistic.  Below is a histogram of 10,000 simulations of $S$.  It definitely is not normally distributed!  How can we see this? Don't use a Z-table or 1.96 for this statistic; even if it did have central limit behavior (which it doesn't!), computing the expectation and standard error in order to get a Z-statistic would be really really hard. 

```{r}

# First, write a function to simulate S
simulate_S = function(){
  S = sample(c("H","T"), 200,replace=T) %>% longestRun()
  return(S)
}

r = 1000
monte_carlo = data.frame(replicate = 1:r, 
                         S = rep(NA,r), 
                         S_in_A = rep(NA, r)) 

for(i in 1:r){
  monte_carlo$S[i] = simulate_S()
}
hist(monte_carlo$S, breaks = 20)
lines(c(5,5), c(0,100000), col = "red", lwd = 3)
```

There is a red line at 5, the length of the longest run in the sequence your classmate gave you.  So, most simulated values of $S$ are larger than 5.  This is consistent with what was stated above... "humans have a hard time imagining what that length should be. As a result, fraudulent sequences have runs which tend to be too short."  So far, it looks like that might be the case for this sequence.

The p-value tells us exactly how surprising $s= 5$ is under the null hypothesis. It is the probability of observing $S$ *equal to, or more extreme* than our observed value.

\[\mbox{p-value} = P(S\le 5).\]

Why "or more extreme"?  

[deep breath]

Before learning about Monte Carlo, if someone asked you to compute some weird probability like $P(S\le 5)$ where $S$ is the longest run in a sequence of 200 coin flips, what would you have done?

Of course, now you know Monte Carlo.  So, you know how to compute these probabilities! In what fraction of our simulations is $S\le 5$?

```{r}
# Second, write a function to evaluate whether X \in A.
check_if_S_in_A = function(S){
  return(S  <= 5)
}

for(i in 1:r){
  monte_carlo$S_in_A[i] = check_if_S_in_A(monte_carlo$S[i])
 
}

monte_carlo = as_tibble(monte_carlo)
p_value = monte_carlo %>% summarise(mean(S_in_A))
```

This Monte Carlo probability is a p-value (that's why I called the variable ```p_value```).  It is equal to `r p_value`, which many would say is "statistically significant" because it is less than .05; this .05 number is culturally important to many researchers, but there is nothing mathematically important or special about the .05 cutoff. 

Because we observe a longest run which is improbable under the null, we "reject the null hypothesis".  

What if the longest run was 4?  What if the longest run was still 5, but the sequence of flips was 10,000 instead of 200?  How would we compute p-values for these scenarios?

What other patterns could we examine besides the longest run and proportion of H's?  


## Neyman-Pearson framework (logic of statistical testing)

_(below notes taken from Karl Rohe)_

Here is the outline for how we do these problems:   

1. Specify a "null" model where "nothing is going on" in the experiment and the results of the experiment should be ignored.  In particular, you will need **a way to simulate the process** under this null model.
2. Come up with a test statistic that can be computed from your simulated data as $S$. Identify the types of values of $S$ that are (i) surprising under the null and (ii) expected in your observed data. Usually, this is values of $S$ that are large, or small, or both (i.e. a "two-sided test").
   - Note this means you should expect $S$ to be **different** under the null and alternative.
   - The **more different** $S$ is between the two, the better!
3. Collect data from the real world.  Use this data to compute $s$.  State the event that is surprising, e.g. $S\ge s$, $S\le s$, or $S \in \text{Surprising Set}$, where the surprising set is defined with $s$ (e.g. in two-sided tests).
4. Use Monte Carlo simulation to compute $P(S \ge s)$, $P(S \le s)$, or $P(S \in \text{Surprising Set})$.

In this process, we combine our understanding of the world, our data, and our model.  With all three, we make statistical inference.   

The above logic performs a Monte Carlo simulation under a hypothesized reality.  Suppose that you observe a statistic $s$.  Using a simulation of hypothesized reality, we can simulate a random variable $S$ that we *imagine* as mimicking $s$ under the null. Using Monte Carlo, we can compute (approximate) $P(S \ge s)$ or $P(S\le s)$ (i.e. one-sided tests) or any other comparison that seems reasonable given the circumstances (e.g. two-sided tests, etc).  If the null model is unlikely to generate a value equal to $s$, or more extreme, then we reject the null hypothesis.

If you have learned a little bit about hypothesis testing before this class, then the above logic might look very strange.  However, if scientists had "electronic computers" when they first started doing statistics, then this is exactly what they would have done and it is exactly what you would have been taught.  However, when us humans started doing statistics, it was very hard to simulate lots of $S$ values.  So, we came up with mathematical formulas to approximate the above logic.  Those formulas are the things you might have previously learned.  Once you understand the logic above, I hope that you can return to those formulas and find a deeper understanding for them.  


## Example: Car deaths

In 2018,  39,404 people in the US died in car accidents.  In 2019, it was 38,800.  So, it went down by roughly 1.8%. Is this a difference that could have happened by chance?  Or, is this difference "statistically significant"?

Follow the logic of statistical testing above.

1) What is a statistical model for $D_{2018}, D_{2019}$, the number of people that die in the US in car accidents in 2018 and 2019, such that they have the same distribution? These are counts of things.  So, I would propose Poisson.  To embed the notion of "no statistical difference between 2018 and 2019", they should have the same rate parameter $\lambda$.  We will set $\lambda$ to be the average of the two observed values (this is our best guess)... $39,102 = (38,800+39,404)/2$.  Then,

$H_0: D_{2018}, D_{2019}$ are independent and identically distributed Poisson($\lambda = 39,102$)

2) As a test statistic, I propose that we use "the percent difference"

$S = \frac{D_{2018} - D_{2019}}{D_{2018}}.$

Notice that under the null hypothesis, $S$ is going to take values around zero.
I think we should use a "two-sided test", meaning that we will reject if the absolute value of $S$ (i.e. $|S|$) is big. (Why two-sided? Well, if the number was going up, people would also have a strong reaction.  So, whether $S$ is large and positive or large and negative, we would take either as providing evidence against the null hypothesis.)  So, we will want to evaluate

$P(|S| > |s|)$

3) We have our data, $d_{2018} = 39,404$ and $d_{2019} = 38,800$.  So, 

$s = \frac{d_{2019} - d_{2018}}{d_{2018}}\approx -1.5\%$

```{r}
s = (38800-39404)/(39404)
s
```

4) 

```{r}
library(tidyverse)

# First, write a function to simulate S
simulate_S = function(){
  D2018 = rpois(1,39102)
  D2019 = rpois(1,39102)
  S = (D2019-D2018)/(D2018)
  return(S)
}
# Second, write a function to evaluate whether S \in A.
check_if_S_in_A = function(S){
  return(abs(S) > abs(s))
}
# Now, we are going to do it lots of times.  
# Let's arrange the simulations in a data.frame with three columns
r = 10000
monte_carlo = data.frame(replicate = 1:r, 
                         S = rep(NA,r), 
                         S_in_A = rep(NA, r)) 
for(i in 1:r){
  # monte_carlo$S[i] = simulate_S()
  # monte_carlo$S_in_A[i] = check_if_S_in_A(monte_carlo$X[i])
 
# I'm going to use the alterative way...
 monte_carlo[i,2] = simulate_S()
 monte_carlo[i,3] = check_if_S_in_A(monte_carlo[i,2])
}
monte_carlo = as_tibble(monte_carlo)
monte_carlo %>% summarise(mean(S_in_A))
```


```{r}
monte_carlo %>% ggplot(aes(x = S)) + geom_histogram()
```


## Permutation test

Permutation tests are a subclass of Monte Carlo testing methods in which data are **compared to shuffled versions** of itself. This can be done with a single data set, with two sets of data to be compared, or with even larger more complex datasets.

### Example: comparing two datasets

<!-- https://bookdown.org/curleyjp0/psy317l_guides5/permutation-testing.html-->

Similar to question in discussion, this is useful for comparing two sets of data to test for differences in them.

Suppose you have the following two sets of data from two arms of a clinical trial where the side effect of a new drug is tested on some unnamed health metric:

```{r}
control = c(65, 74, 73, 83, 76, 65, 86, 70, 80, 55, 78, 78, 90, 77, 68)
treatment = c(72, 66, 71, 66, 76, 69, 79, 73, 62, 69, 68, 60, 73, 68, 67, 74, 56, 74)

tibble(values = c(control,treatment),
       group = c(rep("control",15), rep("treatment", 18))) %>% 
  ggplot(aes(x = group, y = values, fill = group)) +
  geom_boxplot(alpha=.3, outlier.shape = NA) +
  geom_jitter(width=.1, size=2)
```

You want to compare to see if the two groups have significantly different means. A null hypothesis that makes sense might be:

$$H_0:\text{Groups have same mean}$$

You can imagine if the two groups indeed have the same mean, then a process where we shuffle the data by randomly assigning it to each group should NOT affect their means. Therefore, a permutation would be an appropriate test to apply here.

```{r}
allscores = c(control,treatment)
N = 10000
results = rep(NA,N)
for(i in 1:N){
  x = split(sample(allscores), rep(1:2, c(15,18)))
  results[i] = mean(x[[2]]) - mean(x[[1]])  
}
summary(results)
```

```{r}
hist(results)
lines(rep(mean(treatment)-mean(control),2),c(0,1300),col="red",lwd=3)
```

Compare with our mean

```{r}
mean(treatment)-mean(control)
```

Which tail do we want? Depends on our question.

For this example, suppose we want to know if the new drug has ANY adverse effects on this health metric. I.e. if it makes it unusually high OR unusually low, we want to be able to detect that effect and report it. Then in that case it makes sense to look at both tails. This is usually defined by centering the data at 0 and then looking for region where

$$P(|S|>s_{obs})$$

When is a one-tailed test appropriate?

> Because the one-tailed test provides more power to detect an effect, you may be tempted to use a one-tailed test whenever you have a hypothesis about the direction of an effect. Before doing so, consider the consequences of missing an effect in the other direction.  Imagine you have developed a new drug that you believe is an improvement over an existing drug.  You wish to maximize your ability to detect the improvement, so you opt for a one-tailed test. In doing so, you fail to test for the possibility that the new drug is less effective than the existing drug.  The consequences in this example are extreme, but they illustrate a danger of inappropriate use of a one-tailed test.
> 
> So when is a one-tailed test appropriate? If you consider the consequences of missing an effect in the untested direction and conclude that they are negligible and in no way irresponsible or unethical, then you can proceed with a one-tailed test. For example, imagine again that you have developed a new drug. It is cheaper than the existing drug and, you believe, no less effective.  In testing this drug, you are only interested in testing if it less effective than the existing drug.  You do not care if it is significantly more effective.  You only wish to show that it is not less effective. In this scenario, a one-tailed test would be appropriate. ^[https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/]



## Example 3: hot hands

For the next example, let's do something slightly more complicated.

A certain professional basketball player believes he has "[hot hands](https://en.wikipedia.org/wiki/Hot_hand)" when shooting 3-point shots (i.e. if he makes a shot, he’s more likely to also make the next shot). His friend doesn’t believe him, so they make a wager and hire you, a statistician, to settle the bet.

As a sample, you observe the next morning as the player takes the same 3-point shot 200 times in a row (assume he is well rested, in good physical shape, and doesn’t feel significantly more tired after the experiment), so his level of mental focus doesn’t change during the experiment). You obtain the following results, where Y denotes a success and N denotes a miss:

```
YNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNNNYNNYYYYNNYYNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNNYNNNNNNYNNNYNNYNNNNNYNYYYNNYYYNYNNNNYNNNNNNNYYNNYYNNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNNNYYYYYYNYYNNYN
```

Note that the existence of a "hot hands" effect means the shots are not indepedent. Also note that there's a third possibility: that the player is more likely to "[choke](https://en.wikipedia.org/wiki/Choke_(sports))" and miss the next shot if he scored the previous one (e.g. maybe scoring a shot makes him feel more nervous because he feels like he's under pressure).

### Attempt 1: run length

Since the existence of a hot hands effect tends to increase the run lengths of `Y`s compared to if the shots were independent, we can use the longest run length as a way of comparing independence vs hot hands (note if the player is a choker, they will tend to have shorter runs of `Y`s than if they were independent, so you can simply ignore this case for now and compare hot hands v. independence for simplicity).

Now, how exactly do you compare these two situations and determine which is a better fit for the data?

One thing that's worth noting is that ***if a sequence of repeated experiments is independent, then it shouldn't matter what order the results are in***. This should be fairly easy to understand and agree with.

Let's ***assume that the throws are totally independent***. Recall we also assume he doesn't get tired so his baseline shot-making ability doesn't change over the course of the experiment. Therefore, we should be able to (under these assumptions) ***arbitrarily reorder his shots without affecting any statistical properties of his shot sequence***. So let's do that!

We begin by parsing the throws into a vector of `Y` and `N`.


```{r}
# the sequence of throws is broken up into 4 chunks for readbility, then
# paste0 is used to merge them into a single sequence, then
# strplit("YN...N",split="") is used to split the string at every "", so
# we get a vector of each character, and finally
# [[1]] is used to get the vector itself (strsplit actually outputs a list
# with the vector as the first element; [[1]] removes the list wrapper)
# 
# for more info about the strsplit function, see
# https://www.journaldev.com/43001/strsplit-function-in-r

throws = strsplit(
   paste0("YNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNN",
          "NYNNYYYYNNYYNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNN",
          "YNNNNNNYNNNYNNYNNNNNYNYYYNNYYYNYNNNNYNNNNNNNYYNNYY",
          "NNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNNNYYYYYYNYYNNYN"), split="")[[1]]

throws
```


Next, we write a function to get the longest run of `Y`s in the throw sequence. Here we use a convenient function called `rle( )` which is short for [run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding), which turns our sequence of throws into sequences of runs (e.g. YNNNNYYNNNY becomes something like "1 `Y`, 4 `N`s, 2 `Y`s, 3 `N`s, and 1 `Y`"). We can then simply take the longest of the `Y` runs.


```{r}
longestRun = function(x,target = 'Y'){
    max(0,with(rle(x), lengths[values==target]))
}

longestRun(throws)
```


Now, we randomly shuffle the sequence of throws many times and see what the longest `Y` runs look like for these shuffled sequences.


```{r}
# set number of reps to use
N = 10000

# create vector to save results in
mc.runs = rep(NA,N)

# for each rep, randomize sequence and find longest run of Y
for(i in 1:N){
    mc.runs[i] = longestRun(sample(throws))
}
```


```{r}
options(max.print=500)
mc.runs
barplot(table(mc.runs))

# use 5.5 here because using 6 plots line at one edge instead of at center
lines(c(5.5,5.5),c(0,4000),col="red",lwd=3)
```


```{r}
hist(mc.runs)
```


compared to other shuffled sequences, our run length doesn't seem that unlikely. Therefore, this method seems inconclusive.

Can we find an even better "statistic" to use?

### Attempt 2: running odds ratio

Consider **every pair of consecutive throws** and make a table of the outcomes. For example, the first 8 throws in the sequence are YNNNNYYN. Breaking this into consecutive pairs, we have YN, NN, NN, NN, NY, YY, YN. This gives the table:

<center>
<div style="width:100px;">

| NN | NY | YN | YY |
|:--:|:--:|:--:|:--:|
| 3  | 1  | 2  | 1  |

</div>
</center>

Suppose we do this for the entire sequence of 200 throws (note this gives you 199 pairs). If we **divide the number of NY by the number of NN**, we get an estimate for **how much _more_ likely he is to make the next shot _assuming he missed his last shot_**.

Similarly, we can **divide the number of YY by the number of YN** to get an estimate for **how much _more_ likely he is to make the next shot _assuming he scored his last shot_**.

Now, note that **if the "hot hands" effect really exists** in the data, then **YY/YN should be larger than NY/NN** in a large enough sample. We use this fact to define the following quantity:

$$R=\frac{(\text{# of YY})/(\text{# of YN})}{(\text{# of NY})/(\text{# of NN})}$$

The ratio $R$ represents, in some sense, **how much more likely** the player is to **make the next shot** if he **made the previous shot _vs_ if he didn't make the previous shot** (note the **_vs_**). This is exactly what we're trying to investigate!

If there is a "hot hands" effect, the numerator should be greater than the denominator and we should have $R>1$. If the throws are independent and do not affect each other then in theory we should have $R=1$. If the player is actually a choker (i.e. he is more likely to miss after a successful shot), then we should have $R<1$. (Side note: this is basically an [odds ratio](https://journalfeed.org/article-a-day/2018/idiots-guide-to-odds-ratios)).

Now, we can use the same general method as the first attempt. If we assume his throws are independent and his shot probability doesn't change significantly during the experiment, then we can randomly shuffle his throws and no properties should change. So let's do that!

First, I wrote a function to split the sequence of throws into consecutive pairs and then tabulates them.


```{r}
# install the runner package if necessary
if(!"runner" %in% rownames(installed.packages())) install.packages("runner")

# define function for tabulating consecutive pairs
tableOfPairs = function(vec){
  return(table(runner::runner(vec,k=2,f=paste,collapse="")[-1]))
}

# test function for correct output
tableOfPairs(strsplit("YNNNNYYN",split="")[[1]])
```



```{r}
# run function on original sequence of throws
tableOfPairs(throws)
```

Next, I wrote a function that takes the above table as an input and returns the ratio R as defined above.


```{r}
ratioFromTable = function(tb){
  return(setNames((tb["YY"]/tb["YN"])/(tb["NY"]/tb["NN"]),"R"))
}

# run on our data
ratioFromTable(tableOfPairs(throws))
```


```{r}
# we can check this is correct by manually computing it
(28/35)/(34/102)
```


Now we just need to shuffle the sequence and see what this ratio looks like for other sequences.


```{r}
# set number of reps to use
N = 10000

# create another vector to save results in
mc.ratios = rep(NA,N)

# for each rep, randomize sequence and find ratio R
for(i in 1:N){
    mc.ratios[i] = ratioFromTable(tableOfPairs(sample(throws)))
}
```


```{r}
options(max.print=500)
round(mc.ratios,2)
```


```{r}
hist(mc.ratios)
lines(c(2.4,2.4),c(0,2600),col="red",lwd=3)
```

Now we can see our original ratio of $R=2.4$ seems extremely unlikely! In particular, most of the shuffled statistics are centered around 1 (which is what we expect, since we established $R=1$ for independent sequences).

This method (which is a little more refined than the simpler run length method) appears to show that our original sequence isn't well explained by the throws being independent. Since $R=2.4\gg1$ and this result appears unlikely to happen under independence, we may conclude **the player does actually have hot hands**.

<br/>
<br/>
<br/>
<br/>
<br/>

---

#### _Appendix: Comment about how the throws were generated_

As you may have guessed, the throws were intentionally generated to have a "hot hands" effect. The first throw was randomly chosen with a $35\%$ chance of success (chosen based on a quick Google search of average 3-point shot rates). After that, the probability of success of the next shot was dependent on the success of the previous shot. If the previous attempt was a success, this was raised to $45\%$; if it missed, this was lowered to $25\%$.

```{r}
n = 200

throws = rep(NA,n)

for(i in 1:n){
  if(i==1){
    throws[i] = sample(c("Y","N"),1,prob=c(0.35,0.65))
  } else {
    if(throws[i-1]=="Y"){
      throws[i] = sample(c("Y","N"),1,prob=c(0.45,0.55))
    } else{
      throws[i] = sample(c("Y","N"),1,prob=c(0.25,0.75))
    }
  }
}
```

The numbers were chosen this way so that the average number of 3-point shots made is still close-ish to $35\%$ (it's $31.25\%$ to be exact) and so that the effect is large enough to be detectable, but not so much so that the run-length statistic will also be able to detect an effect (which might happen if the strength of the effect was raised by raising/lowering the numbers even more).

200 was used as the number of throws in the experiment as a balance between lowering the variance of the ratio $R$ of the generated sequence while still being somewhat plausible for a professional basketball player to achieve.



##  Extra example (if there is time): Coin flips  

A friend wants to gamble with you and proposes a coin flip game.  Before playing the game with her, you want to ensure whether the coin is fair.  


#### Suppose you have the patience to flip it 1000 times and it comes up heads 54\% of the time.  Do you think the coin is fair?  

Let's follow the logic above...

1) If we are going to use null hypothesis testing to address this problem, then the null model for the coin is that it is a fair coin.  
\[\mbox{$H_0$: Each coin flip comes up heads independently, with probability 1/2.}\]
2) Let $X$ denote the proportion of flips that are heads. The types of values that would be surprising are values away from 1/2.  So, to make our life easier we could define $S = X - 1/2$.

```{r}
simulate_S = function(){
  
  lots_of_random_variables = rbinom(n = 1000, size = 1, p = 1/2)
  X = mean(lots_of_random_variables)
  
  S = X - 1/2
  return(S)
}
```

Then, we would find large values of $S$ to be surprising; large and positive **or** large and negative.  This is a 
"two-sided test". 

3) The problem gives us that $x = .54$ and $s= .54 - .5 = .04$; notice that this is lower case $x$ and $s$ because it is the data we've observed.  So, the "surprising event" is 
\[\{\mbox{Either } S \ge .04 \ \mbox{ or }  \ S \le -.04\}= \{|S| > .04\}\]

```{r}
check_if_S_in_surprising_set = function(S){
  return(abs(S) >= .04)
}
```

4) Use Monte Carlo simulation to compute 
$$P(S \ge .04 \ \mbox{ or }  \ S \le -.04) = P(|S| \ge .04)...$$ 

```{r}
library(dplyr)
r = 10000
monte_carlo = data.frame(replicate = 1:r, 
                         S = rep(NA,r), 
                         S_in_suprising_set = rep(NA, r)) 
for(i in 1:r){
  monte_carlo$S[i] = simulate_S()
  monte_carlo$S_in_suprising_set[i] = check_if_S_in_surprising_set(monte_carlo$S[i])
 }
monte_carlo = as_tibble(monte_carlo)
monte_carlo %>% summarise(mean(S_in_suprising_set))
```

So, if the coin were fair, this would happen only about 1\% of the time. So, I would say that what we've observed (54\% heads) is pretty surprising!  

#### What if the coin came up heads 52\% of the time out of 1000 flips?  

What has changed in the logic above?  

Here is the simulation...

```{r}
check_if_S_in_surprising_set = function(S){
  return(abs(S) >= .02)  # why .02????
}
library(dplyr)
r = 10000
monte_carlo = data.frame(replicate = 1:r, 
                         S = rep(NA,r), 
                         S_in_suprising_set = rep(NA, r)) 
for(i in 1:r){
  monte_carlo$S[i] = simulate_S()
  monte_carlo$S_in_suprising_set[i] = check_if_S_in_surprising_set(monte_carlo$S[i])
 }
monte_carlo = as_tibble(monte_carlo)
monte_carlo %>% summarise(mean(S_in_suprising_set))
```

So, if the coin were fair, then roughly 20\% of experiments would create something at least as surprising as 52\% heads.  I don't think this is that surprising.  Does this mean that the coin is fair?  No.  It means that we don't have enough evidence to suggest it is unfair.  That is, we don't "accept the null hypothsis (fair coin)". We never "accept" the null hypothesis.  

#### What if the coin came up heads 52\% of the time out of 10000 flips (instead of 1000)?


What has changed in the logic above?  

Compared to the previous question, we have the same proportion (52\%), but 10 times as may samples. 

Here is the simulation...

```{r}
simulate_S = function(){
  
  lots_of_random_variables = rbernoulli(n = 10000, p = 1/2)
  X = mean(lots_of_random_variables)
  
  ## or, you could write:
  # X = rbinom(n = 1, size = 10000, prob = 1/2) / 10000
  
  S = X - 1/2
  return(S)
}
r = 10000
monte_carlo = data.frame(replicate = 1:r, 
                         S = rep(NA,r), 
                         S_in_suprising_set = rep(NA, r)) 
for(i in 1:r){
  monte_carlo$S[i] = simulate_S()
  monte_carlo$S_in_suprising_set[i] = check_if_S_in_surprising_set(monte_carlo$S[i])
 }
monte_carlo = as_tibble(monte_carlo)
monte_carlo %>% summarise(mean(S_in_suprising_set))
```

Ten times more samples makes 52\%, 1000 times less likely!  Whoa.  

#### How many times would you need to flip the coin to be certain that it is fair?

Discuss...  What is a more productive variation on this question?



<!-- ## Example: Odd Birthdays  -->

<!-- In the logic above, it looks very clear... steps 1, 2, 3, 4.  This is an ideal.  Often times, it is more iterative, back-and-forth.  For example, it might look like this... -->

<!-- January 1 is an odd date, because 1 is odd.  What percentage of students are born on odd dates?  Given that half of the counting numbers are odd, let's test the null hypothesis -->
<!-- $H_0: \mbox{half of students have odd birthdays}$ -->
<!-- by collecting some data. For example, the data that I imagine observing could look like this: -->

<!-- ```{r} -->
<!-- # enter the data: -->
<!-- odd = 20 -->
<!-- even = 15 -->
<!-- proportionOdd = odd/(odd+even) -->
<!-- proportionOdd -->
<!-- ``` -->

<!-- The proportion have odd birthdays will be our test statistic $S$. -->

<!-- In class, we will collect some real data. It might look like we are ready to do that now.  However, the fundamental thing from steps 1 and 2 is that we are able to simulate $S$ under the null hypothesis.   -->

<!-- How would we do that? The null hypothesis does not really create a statistical model for data we observe. We will need to clarify it...   -->

<!-- Perhaps we think the code below can clarify our statistical model (step 2 before step 1?).  Well, it certainly expresses the null/imagined model **exactly**. -->

<!-- ```{r} -->
<!-- library(purrr) -->
<!-- simulate_S = function(){ -->
<!--   which_birthdays_are_odd = rbernoulli(n = 35, p = .5)   # why 35?  why .5? -->
<!--   oddBirthdays = sum(which_birthdays_are_odd) -->
<!--   proportionOdd = oddBirthdays/35 -->
<!--   S = proportionOdd -->
<!--   # # rbernoulli comes from purrr.  -->
<!--   # # if you prefer very concise code -->
<!--   # # you could write: -->
<!--   # X = rbinom(n = 1,size = 35,prob = .5)/35 -->
<!--   #  note that this concise code doesn't depend on purrr  -->
<!--   return(S) -->
<!-- } -->
<!-- ``` -->

<!-- This simulation model actually makes another modeling assumption that we haven't discussed! It is one of the most important assumptions in all of statistics *and it is always hidden*. It's an assumption that we always need to talk about in the context of statistical testing and p-values. In the long tradition of hiding the discussion of this assumption, you will need to find it in this chapter.   -->


<!-- In our model (i.e. in our imagination), we can generate lots of simulated/imagined data sets that have the same sample size as our observed data.  Then, we are going to Monte Carlo (yes, it is now a verb) with $S$ = `proportionOdd` and $A =$ ? to compute $P(S ?)$. -->


<!-- In the end, we will have data.  We want to know something about the world that generated that data. This is inference ("going up").  To do *statistical inference* we need a statistical model and we are going to do what Jordan calls *reductio ad unlikely* (be sure to read the excerpt from Jordan's book).  In this example, the steps of the logic are all jumbled up.  Sometimes, when you are still finding your way, this is what it looks like.  -->


<!-- **Statistical Independence** -->
<!-- In order to generate more than one birthday, the code above "simply generates 35 birthdays".  The assumption is inside of that. -->

<!-- This implicitly models the birthdays as "independent".  This means that if you know some set of the birthdays, you are not able to predict predict anything about the other birthdays. For this example, in the real world, I would call this a "reasonable assumption that is definitely wrong".  Why is the assumption wrong?  Why is it still reasonable? -->

## Finally: some more terminology

See below table:

||Reject H0|Not Reject H0|
|---|---|---|
|H0 True|Type I Error ($\alpha$)|Correct|
|H0 False| Correct | Type II Error ($\beta$)|

If the null is true and we do not reject it, or if the null is false and we reject it, we have done the right thing. If the null is true and we reject it, we have made the wrong conclusion. We say we have made a Type I error, or an $\alpha$ error. We also call $\alpha$ the probability of a Type I error, which can be expressed as follows:
$$\alpha= P(\text{Reject}~H_0|H_0~\text{true})$$

If the null is false and we do not reject it, we have also made the wrong conclusion. We say we have made
a Type II error, or a $\beta$ error. We also call $\beta$ the probability of a Type II error, which can be expressed as
follows:
$$\beta = P(\text{Not reject}~H_0|H_0~\text{false})$$

Very closely related to $\beta$ is power:
$$Power = 1-\beta= P(\text{Reject}~H_0|H_0~\text{false})$$
